{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two different ways to model things\n",
    "\n",
    "When considering a problem of process modeling we - usually - have set of N data points pairs ${(x_n, y_n)}; 1 \\le n \\le N$. This dataset comes from observing some process out in the wild. What we would like to do is to build mapping $f$ that will approximate the very process we observed. \n",
    "\n",
    "We will compare two approaches - quite popular **regression** and the topic of this presentation **ordinary differential equations**.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We can write regression in general from as:\n",
    "$$Y = f(X, \\beta) + \\epsilon$$\n",
    "\n",
    "Of course, $f$ can take various form - from simple matrix multiplication to extremally complex like complicated neural nets. What is most important is the fact that we describe the relationship _directly_. \n",
    "\n",
    "\n",
    "### Ordinary Differential Equations\n",
    "\n",
    "On the other hand, when using the ODE approach instead of using a _direct_ approach, we can observe _rate of change_ - which in mathematical terms means we are toying with derivatives. We can describe this relationship as:\n",
    "$$\\frac{dy}{dx} = f'(x),$$\n",
    "where f is a function we are trying to approximate. To calculate $f$, we need to integrate it.\n",
    "\n",
    "The general idea is that instead of approximating the function itself, we can approximate it's derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODE Refresher\n",
    "\n",
    "### What are ODEs?\n",
    "Straight from Wikipedia:\n",
    "> In mathematics, an ordinary differential equation (ODE) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.\n",
    "\n",
    "Let $y = f(x)$ then equation of the form:\n",
    "$$F(x, y, y', ..., y^{(n-1)})=y^{(n)}$$\n",
    "is called explicit ordinary differential equation of order n.\n",
    "\n",
    "### Initial value problem in ODEs\n",
    "\n",
    "One of the most common problems is that of _initial value problem_. When modeling system this usually means that we want to know how a system will evolve in \"time\" given some initial conditions.\n",
    "\n",
    "#### Sample Problem\n",
    "\n",
    "Example taken from [here](http://tutorial.math.lamar.edu/Classes/DE/Modeling.aspx).\n",
    "\n",
    "_A population of insects in a region will grow at a rate that is proportional to their current population. In the absence of any outside factors, the population will triple in two weeks. On any given day there is a net migration into the area of 15 insects and 16 are eaten by the local bird population and 7 dies of natural causes. If there are initially 100 insects in the area will the population survive? If not, when do they die out?_\n",
    "\n",
    "###### General Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Nice implementation of specific problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet and Euler's Method\n",
    "\n",
    "### What is ResNet?\n",
    "\n",
    "ResNet is a network architecture introduced in 2015, taking _deep_ in deep learning to a whole new level. The motivation behind creating this network is highly empirical - creators (not only them) observed issue the named _degradation problem_. The observed that adding layers not only do not improve the overall accuracy of the network but worsens it. That implies that added layers are not able to learn even identity function - which would be enough to keep accuracy on the same level.\n",
    "\n",
    "Let's set that ordered set of layers should should learn some function $g$. If our net is able to learn how to approximate this function then it should be able to learn how to approximate residual function $f(x) = g(x) - x$. From that we have $g(x) = f(x) + x$\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "Residual block can be considered as a mini neural net, where _input of this net is also added to the output of the net_. We may think about it as:\n",
    "$$fr^l(x) = f(x) + Wx$$\n",
    "Where $l$ represents a number of layers in a mini neural net. $W$, on the other hand, is a matrix that allows simple linear transformation, for example, to make \"dimensions compile\". Usually, it's just an identity matrix.\n",
    "\n",
    "#### Simple implementation of residual block\n",
    "TODO\n",
    "\n",
    "#### ResNet as Euler Discretization\n",
    "\n",
    "Now... let's get back to ODEs. One of the simplest approaches to numerically solve ODE is _Euler's Discretization_. Idea behind this method goes as follows:\n",
    "1. Consider $y' = f(x, y)$ given initial condition $y_0 = y(x_0)$\n",
    "2. Let's suppose our step size is of size $t$. We will use this value to update value of $x$: $x_{n+1} = x_{n} + t$.\n",
    "3. From definition of derivative we have $y' = \\frac{\\Delta y}{t}$, simple manipulation gives us $\\Delta y = tf(x_n, y_n)$\n",
    "4. From everything above we get $y_{n+1} = y_n + \\Delta y$ and $y_{n + 1} = y_n + tf(x_n, y_n)$.\n",
    "\n",
    "It turns out we can eaisly represent ResNet block in similar form, particularly like this:\n",
    "$$h(t+1) = h(t) + f(h(t))$$\n",
    "which is extremally similar to:\n",
    "$$y_{n+1} = y_n + \\Delta y$$\n",
    "\n",
    "Hmm... \n",
    "\n",
    "#### Making Discretization continous\n",
    "\n",
    "Authors of Neural ODE asked a question (not first though) and provided implementable answers to the following question: _what will happen if we make as small steps as possible?_ Making our discretization - continous. They described the state of the neural network, or rather change of the state in the following form:\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(t, h(t); \\theta_t)$$\n",
    "\n",
    "Where t is _depth_ of our network, which could be imagined as continous number of layer. So $h(t_0)$ would be input to the network and $h(t_1)$ could be the output of the network. \n",
    "\n",
    "##### Forward pass \n",
    "\n",
    "For starter let's think how to do forward pass in a network defined as above. The most straight forward approach seems to make sense:\n",
    "\n",
    "$$h(t_1) = h(t_0) + \\int_{t_0}^{t_1}f(t, h(t); \\theta_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "### Naive Approach\n",
    "\n",
    "### Adjoint Sensitivity Method\n",
    "\n",
    "#### Sample Problem\n",
    "\n",
    "#### Final backprop algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODE - summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple problem - CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    " - https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2019-03-Neural-Ordinary-Differential-Equations\n",
    " - https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html\n",
    " - https://arxiv.org/abs/1806.07366\n",
    " - https://pl.wikipedia.org/wiki/Regresja_(statystyka)\n",
    " - https://en.wikipedia.org/wiki/Ordinary_differential_equation\n",
    " - https://en.wikipedia.org/wiki/Initial_value_problem\n",
    " - http://tutorial.math.lamar.edu/Classes/DE/Modeling.aspx\n",
    " - Reprezentacja stanu sieci neuronowych przy pomocy zwyczajnych równań różniczkowych - Piotr Lewandowski"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
